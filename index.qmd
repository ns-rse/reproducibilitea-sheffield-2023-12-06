---
title: "ReproducibiliTea Sheffield Journal Club 2023-12-06"
# format: revealjs
author:
  - name: Neil Shephard
    orcid: 0000-0001-8301-6857
    email: n.shephard@sheffield.ac.uk
    affiliations: RSE, The University of Sheffield
from: markdown+emoji
format:
  clean-revealjs:
    incremental: false
    slide-number: true
    show-slide-number: speaker
    auto-stretch: false
    chalkboard: true
    scrollable: true
    # embed-resources: true
    # standalone: true
revealjs-plugins:
  - confetti
footer: "**Slides** : [**ns-rse.github.io/reproducibilitea-sheffield-2023-12-06**](https://ns-rse.github.io/reproducibilitea-sheffield-2023-12-06)"
project:
  preview:
    port: 7864
    host: localhost
    watch-inputs: true
---

## Scan This!

{{< qrcode https://ns-rse.github.io/reproducibilitea-sheffield-2023-12-06 qr1 width=400 height=400 >}}

[ns-rse.github.io/reproducibilitea-sheffield-2023-12-06](https://ns-rse.github.io/reproducibilitea-sheffield-2023-12-06)

## High replicability of newly discovered social-behavioural findings is achievable. [10.1038/s41562-023-01749-9](https://doi.org/10.1038/s41562-023-01749-9)

> Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to
> suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper
> reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental
> findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological
> transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%,
> replication attempts here produced the expected effects with significance testing (P < 0.05) in 86% of attempts,
> slightly exceeding the maximum expected replicability based on observed effect sizes and sample sizes. When one lab
> attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that in the
> original study. This high replication rate justifies confidence in rigour-enhancing methods to increase the
> replicability of new discoveries.


::: {.notes}
This is the abstract for today's paper.
:::

## Overview

+ Four psychology laboratories.
+ Each conducted Pilot Studies > Self-Confirmatory > Replication
+ Prior experience of replication efforts.
+ Study criteria...
  + Two-group between subject manipulation
  + One dependent variable
  + Administered via computer online to adults in 20 minutes session
  + Self-selecting volunteers signed-up to complete online surveys

::: {.notes}

:::

## Reults

![[Figure 1 : Effect size estimates and 95% CI from 16 new discoveries with four replications
each.](https://www.nature.com/articles/s41562-023-01749-9/figures/1)](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41562-023-01749-9/MediaObjects/41562_2023_1749_Fig1_HTML.png?as=webp){width=800
height=550}


## Good


1. Pilot studies pre-registered (procedure, hypothesis, analysis plan, exclusions) /before/ data collection.
2. Large sample size
3. Clear protocols produced for sharing
4. Replication population very similar to confirmatory - targeted.
5. Open Science! All materials of individual studies appear to be available via [Open Science Framework](https://osf.io)

## Unusual

1. Not all pilots proceeded to confirmation/replication (selection bias?)
2. Clear protocols still need clarification in many cases (only 14/48 29% remained independent)
3. Null confirmatory that had significant results in confirmatory was counted as successful
   replication?!?!. ([Bak-Coleman & Devezer](https://europepmc.org/article/PPR/PPR763078))
4. Replication estimate of 86% is across all studies, normally meta-analysis is conducted within study to say whether it
   replicates.
5. Replication population very similar to confirmatory - will effects generalise?


::: {.notes}
The paper states that there was a pilot phase for all of the studies, but not all pilots proceeded to the
self-confirmatory or replication stage. This suggests there may be some selection bias in the studies that were used as
a basis for estimating replication. I would have preferred to have seen a set of hypotheses (16 or more) randomised to
each of the four centers.

I also found it surprising that whilst protocols were pre-registered there was a high number of centers seeking
additional information 34 out of 48 replications sought clarification with only 14 (29%) not seeking any. It was mostly
UVA who refrained from seeking clarification. Does state that many were trivial but not always the case. Whether this
was deliberate through a strict adherence to the meta-protocol or lack of detail/understanding on the behalf of others
is unclear.

Confirmatory tests that were null and subsequently reached significance in self/independent replication were counted as
replication which is bizzare, I would have expected replication to be demonstrating the same result as the original
effect size. Strangely they focus on the 13 self-confirmatory that were positive and then assess replication as being
90% in that subset, but ignore the three where the self-confirmatory was null and subsequent replication!

Normally replication is via meta-analysis within hypotheses, not across, so this estimate is likely not representative
of how effective the practices are. Really this is more an estimate of the sensitivity (true positive rate) of science
wide studies.
:::


## Unsurprising

+ Large sample sizes give more robust findings.
+ Replication requires clear and accurate protocols.
+ Useful to get clarification when protocols aren't clear.
+

::: {.notes}
Its not surprising to me to find out that large sample sizes give more robust findings, this is fairly basic statistics
as you are less prone to sampling bias when you have a larger sample size.

Not overly surprising that replication requires clear and accurate protcols to be pre-registered either, researchers
need to know what others have done in sufficient detail. Its good that clarification was available when required and
this should be the norm, but how often do authors/researchers contact authors seeking clarification and not hear
anything back?

:::

## Other thoughts

+ Frequentist framework and reporting of p-values/confidence interval
+ Perform meta-analysis within studies
+ Would a Bayesian approach and meta-analysis give

## Links

+ High replicability of newly discovered social-behavioural findings is
  achievable. [10.1038/s41562-023-01749-9](https://doi.org/10.1038/s41562-023-01749-9)
+ Causal claims about scientific rigor require rigorous causal evidence
  [10.31234/osf.io/5u3kj](https://doi.org/10.31234/osf.io/5u3kj)
+ Why Most Published Research Findings Are False [doi.org/10.1371/journal.pmed.1004085](
  https://doi.org/10.1371/journal.pmed.1004085)
+ An estimate of the science-wise false discovery rate and application to the top medical literature
  [doi.org/10.1093/biostatistics/kxt007](https://doi.org/10.1093/biostatistics/kxt007) (see also discussion and
  rejoinder in [_Biostatistics_ Vol 15(1)](https://academic.oup.com/biostatistics/issue/15/1))
